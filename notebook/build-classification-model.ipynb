{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "# from nets import *\n",
    "import time, os, copy, argparse\n",
    "import sklearn.metrics\n",
    "import multiprocessing\n",
    "# from torchsummary import summary\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Downloads/DentalBookObjectDetection/cnncls\n"
     ]
    }
   ],
   "source": [
    "HERE   = 'notebook'\n",
    "ROOT   = os.getcwd().replace(HERE, \"\")\n",
    "SOLUTION = 'cnncls/'\n",
    "os.chdir(os.path.join(ROOT, SOLUTION))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train      = {}\n",
    "validation = {}\n",
    "test       = {}\n",
    "train['directory']      = '../resource/caries/classification/train/'\n",
    "validation['directory'] = '../resource/caries/classification/validation/'\n",
    "test['directory']       = '../resource/caries/classification/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "epoch = 50\n",
    "category = 2\n",
    "# model_name = 'mobilenetv2'\n",
    "# model_name = \"resnet18\"\n",
    "# model_name = \"efficientnetb0\"\n",
    "model_name = \"densenet121\"\n",
    "checkpoint = \"checkpoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                      [0.229, 0.224, 0.225])\n",
    "        transforms.Normalize([0.475, 0.475, 0.475],\n",
    "                             [0.165, 0.165, 0.165])\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.475, 0.475, 0.475],\n",
    "                             [0.165, 0.165, 0.165])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.475, 0.475, 0.475],\n",
    "                             [0.165, 0.165, 0.165])\n",
    "    ])    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': datasets.ImageFolder(root=train['directory'], transform=transform['train']),\n",
    "    'validation': datasets.ImageFolder(root=validation['directory'], transform=transform['validation']),\n",
    "    'test': datasets.ImageFolder(root=test['directory'], transform=transform['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'carious': 0, 'normal_crevice': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = {\n",
    "    'train':dataset['train'].__len__(),\n",
    "    'validation':dataset['validation'].__len__(),\n",
    "    'test':dataset['test'].__len__()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = {\n",
    "    'train':data.DataLoader(dataset['train'], batch_size=batch, shuffle=True, drop_last=True),\n",
    "    'validation':data.DataLoader(dataset['validation'], batch_size=batch, shuffle=False, drop_last=False),\n",
    "    'test':data.DataLoader(dataset['test'], batch_size=batch, shuffle=False, drop_last=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = models.efficientnet_b7(pretrained=True)\n",
    "# m = list(m.children())[:-1]\n",
    "# m = nn.Sequential(*m)\n",
    "# x = torch.randn((16, 3, 224, 224))\n",
    "# m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(model_name=='efficientnetb7'):\n",
    "\n",
    "    backbone = models.efficientnet_b7(pretrained=True)\n",
    "    feature = list(backbone.children())[:-1]\n",
    "    model = nn.Sequential(*feature, nn.AdaptiveAvgPool2d(1), nn.Flatten(1, -1), nn.Linear(2560, category))\n",
    "    pass\n",
    "\n",
    "if(model_name=='efficientnetb0'):\n",
    "\n",
    "    backbone = models.efficientnet_b0(pretrained=True)\n",
    "    feature = list(backbone.children())[:-1]\n",
    "    model = nn.Sequential(*feature, nn.AdaptiveAvgPool2d(1), nn.Flatten(1, -1), nn.Linear(1280, category))\n",
    "    pass\n",
    "\n",
    "if(model_name=='resnet18'):\n",
    "\n",
    "    backbone = models.resnet18(pretrained=True)\n",
    "    feature = list(backbone.children())[:-1]\n",
    "    model = nn.Sequential(*feature, nn.AdaptiveAvgPool2d(1), nn.Flatten(1, -1), nn.Linear(512, category))\n",
    "    pass\n",
    "\n",
    "if(model_name=='densenet121'):\n",
    "\n",
    "    backbone = models.densenet121(pretrained=True)\n",
    "    feature = list(backbone.children())[:-1]\n",
    "    model = nn.Sequential(*feature, nn.AdaptiveAvgPool2d(1), nn.Flatten(1, -1), nn.Linear(1024, category))\n",
    "    pass\n",
    "\n",
    "if(model_name=='mobilenetv2'):\n",
    "\n",
    "    backbone = models.mobilenet_v2(pretrained=True)\n",
    "    feature = list(backbone.children())[:-1]\n",
    "    model = nn.Sequential(*feature, nn.AdaptiveAvgPool2d(1), nn.Flatten(1, -1), nn.Linear(1280, category))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "##\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, dampening=0, weight_decay=0)\n",
    "# schedule = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "##\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "schedule = None\n",
    "\n",
    "##\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=0.0001, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "# schedule = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, criterion, optimizer, schedule, epoch):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Tensorboard summary\n",
    "    # writer = SummaryWriter()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "\n",
    "        print('Epoch {}/{}'.format(e, epoch - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for mode in ['train', 'validation', 'test']:\n",
    "\n",
    "            if(mode == 'train'): model.train()  # Set model to training mode\n",
    "            if(mode == 'validation'): model.eval()   # Set model to evaluate mode\n",
    "            if(mode == 'test'): model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in loader[mode]:\n",
    "\n",
    "                # inputs = inputs.to(device, non_blocking=True)\n",
    "                # labels = labels.to(device, non_blocking=True)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                if(mode=='train'):\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    pass\n",
    "\n",
    "                if(mode=='validation' or mode=='test'):\n",
    "\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        pass\n",
    "\n",
    "                    pass\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                pass\n",
    "\n",
    "            if(mode == 'train'): \n",
    "                \n",
    "                if(schedule): schedule.step()\n",
    "                pass\n",
    "            \n",
    "            epoch_loss = running_loss / size[mode]\n",
    "            epoch_acc = running_corrects.double() / size[mode]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(mode, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if(mode == 'validation' and epoch_acc > best_acc):\n",
    "                \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                pass\n",
    "\n",
    "            pass\n",
    "\n",
    "        pass\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    best_model = model\n",
    "    best_model.load_state_dict(best_model_wts)\n",
    "    return(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.3843 Acc: 0.8041\n",
      "validation Loss: 0.3332 Acc: 0.8366\n",
      "test Loss: 0.3450 Acc: 0.8612\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.2150 Acc: 0.9011\n",
      "validation Loss: 0.2778 Acc: 0.8742\n",
      "test Loss: 0.2596 Acc: 0.8855\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.1719 Acc: 0.9214\n",
      "validation Loss: 0.2560 Acc: 0.8852\n",
      "test Loss: 0.2808 Acc: 0.8943\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.1355 Acc: 0.9330\n",
      "validation Loss: 0.3027 Acc: 0.8609\n",
      "test Loss: 0.3029 Acc: 0.8744\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.1264 Acc: 0.9357\n",
      "validation Loss: 0.3384 Acc: 0.8433\n",
      "test Loss: 0.3171 Acc: 0.8767\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.1094 Acc: 0.9431\n",
      "validation Loss: 0.2933 Acc: 0.8918\n",
      "test Loss: 0.3220 Acc: 0.8855\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0885 Acc: 0.9516\n",
      "validation Loss: 0.2670 Acc: 0.8830\n",
      "test Loss: 0.3124 Acc: 0.8943\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9549\n",
      "validation Loss: 0.3325 Acc: 0.8874\n",
      "test Loss: 0.3458 Acc: 0.8789\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.9621\n",
      "validation Loss: 0.3508 Acc: 0.8653\n",
      "test Loss: 0.4027 Acc: 0.8634\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0611 Acc: 0.9637\n",
      "validation Loss: 0.2852 Acc: 0.8830\n",
      "test Loss: 0.3530 Acc: 0.8855\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.0562 Acc: 0.9635\n",
      "validation Loss: 0.3075 Acc: 0.9029\n",
      "test Loss: 0.4108 Acc: 0.8855\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0481 Acc: 0.9662\n",
      "validation Loss: 0.3315 Acc: 0.9007\n",
      "test Loss: 0.3937 Acc: 0.8855\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0487 Acc: 0.9684\n",
      "validation Loss: 0.4129 Acc: 0.8609\n",
      "test Loss: 0.3981 Acc: 0.8943\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0499 Acc: 0.9668\n",
      "validation Loss: 0.3393 Acc: 0.8830\n",
      "test Loss: 0.3189 Acc: 0.8899\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0370 Acc: 0.9720\n",
      "validation Loss: 0.3375 Acc: 0.8896\n",
      "test Loss: 0.3944 Acc: 0.8899\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9706\n",
      "validation Loss: 0.3067 Acc: 0.8918\n",
      "test Loss: 0.4200 Acc: 0.8877\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0473 Acc: 0.9676\n",
      "validation Loss: 0.3787 Acc: 0.8808\n",
      "test Loss: 0.3135 Acc: 0.8965\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0426 Acc: 0.9703\n",
      "validation Loss: 0.4228 Acc: 0.8764\n",
      "test Loss: 0.4673 Acc: 0.8744\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0296 Acc: 0.9747\n",
      "validation Loss: 0.3212 Acc: 0.8940\n",
      "test Loss: 0.3281 Acc: 0.9141\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0247 Acc: 0.9761\n",
      "validation Loss: 0.3740 Acc: 0.8918\n",
      "test Loss: 0.3701 Acc: 0.8965\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.0243 Acc: 0.9753\n",
      "validation Loss: 0.3599 Acc: 0.8830\n",
      "test Loss: 0.3736 Acc: 0.8943\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0249 Acc: 0.9758\n",
      "validation Loss: 0.4197 Acc: 0.8653\n",
      "test Loss: 0.4406 Acc: 0.8855\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0291 Acc: 0.9764\n",
      "validation Loss: 0.3100 Acc: 0.9139\n",
      "test Loss: 0.3443 Acc: 0.9031\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0324 Acc: 0.9739\n",
      "validation Loss: 0.3577 Acc: 0.9029\n",
      "test Loss: 0.3439 Acc: 0.8877\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0195 Acc: 0.9791\n",
      "validation Loss: 0.3786 Acc: 0.8985\n",
      "test Loss: 0.3476 Acc: 0.9163\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0229 Acc: 0.9764\n",
      "validation Loss: 0.5553 Acc: 0.8543\n",
      "test Loss: 0.5514 Acc: 0.8700\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0348 Acc: 0.9728\n",
      "validation Loss: 0.3166 Acc: 0.9117\n",
      "test Loss: 0.2773 Acc: 0.9119\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0220 Acc: 0.9780\n",
      "validation Loss: 0.4725 Acc: 0.8698\n",
      "test Loss: 0.4084 Acc: 0.8943\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0217 Acc: 0.9766\n",
      "validation Loss: 0.3956 Acc: 0.8874\n",
      "test Loss: 0.3202 Acc: 0.9163\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0125 Acc: 0.9802\n",
      "validation Loss: 0.4478 Acc: 0.8874\n",
      "test Loss: 0.4323 Acc: 0.9075\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.0255 Acc: 0.9761\n",
      "validation Loss: 0.3675 Acc: 0.9117\n",
      "test Loss: 0.3577 Acc: 0.9273\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0203 Acc: 0.9758\n",
      "validation Loss: 0.4340 Acc: 0.8830\n",
      "test Loss: 0.4053 Acc: 0.8899\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0196 Acc: 0.9777\n",
      "validation Loss: 0.3592 Acc: 0.9227\n",
      "test Loss: 0.4745 Acc: 0.8965\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9753\n",
      "validation Loss: 0.3621 Acc: 0.8985\n",
      "test Loss: 0.3224 Acc: 0.9185\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0246 Acc: 0.9750\n",
      "validation Loss: 0.4569 Acc: 0.8830\n",
      "test Loss: 0.4575 Acc: 0.8855\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0170 Acc: 0.9780\n",
      "validation Loss: 0.4155 Acc: 0.8874\n",
      "test Loss: 0.3913 Acc: 0.9075\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.9810\n",
      "validation Loss: 0.4458 Acc: 0.8985\n",
      "test Loss: 0.4488 Acc: 0.8921\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9802\n",
      "validation Loss: 0.4261 Acc: 0.9095\n",
      "test Loss: 0.4704 Acc: 0.9031\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0147 Acc: 0.9799\n",
      "validation Loss: 0.3922 Acc: 0.9007\n",
      "test Loss: 0.3567 Acc: 0.9075\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.9816\n",
      "validation Loss: 0.4053 Acc: 0.8918\n",
      "test Loss: 0.4331 Acc: 0.9075\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0210 Acc: 0.9758\n",
      "validation Loss: 0.4191 Acc: 0.8985\n",
      "test Loss: 0.4518 Acc: 0.8987\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0288 Acc: 0.9750\n",
      "validation Loss: 0.3500 Acc: 0.9029\n",
      "test Loss: 0.4290 Acc: 0.8921\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 0.9788\n",
      "validation Loss: 0.3305 Acc: 0.9029\n",
      "test Loss: 0.4104 Acc: 0.9097\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0229 Acc: 0.9775\n",
      "validation Loss: 0.3429 Acc: 0.9051\n",
      "test Loss: 0.3875 Acc: 0.9053\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0163 Acc: 0.9788\n",
      "validation Loss: 0.3296 Acc: 0.8985\n",
      "test Loss: 0.4100 Acc: 0.8965\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0128 Acc: 0.9799\n",
      "validation Loss: 0.3578 Acc: 0.9183\n",
      "test Loss: 0.3689 Acc: 0.9031\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0172 Acc: 0.9786\n",
      "validation Loss: 0.3350 Acc: 0.9117\n",
      "test Loss: 0.3652 Acc: 0.9009\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0086 Acc: 0.9816\n",
      "validation Loss: 0.4247 Acc: 0.8940\n",
      "test Loss: 0.4272 Acc: 0.8899\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0148 Acc: 0.9797\n",
      "validation Loss: 0.4832 Acc: 0.8808\n",
      "test Loss: 0.3999 Acc: 0.8767\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.9813\n",
      "validation Loss: 0.4733 Acc: 0.8808\n",
      "test Loss: 0.5147 Acc: 0.8744\n",
      "Training complete in 21m 36s\n",
      "Best val Acc: 0.922737\n"
     ]
    }
   ],
   "source": [
    "better_model = learn(model, criterion, optimizer, schedule, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(checkpoint, exist_ok=True)\n",
    "torch.save(better_model, \"{}/{}.pt\".format(checkpoint, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(model, loader, device='cpu'):\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    truth      = []\n",
    "    prediction = []\n",
    "    score      = []\n",
    "    for inputs, labels in loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        truth      += labels.to('cpu').tolist()\n",
    "        prediction += preds.to('cpu').tolist()\n",
    "        score      += [outputs]\n",
    "        continue\n",
    "    \n",
    "    score = torch.cat(score, 0).to('cpu').numpy()\n",
    "    out = (truth, prediction, score)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet121\n",
      "validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93       255\n",
      "           1       0.91      0.92      0.91       198\n",
      "\n",
      "    accuracy                           0.92       453\n",
      "   macro avg       0.92      0.92      0.92       453\n",
      "weighted avg       0.92      0.92      0.92       453\n",
      "\n",
      "0.9643295702119232\n",
      "[[236  19]\n",
      " [ 16 182]]\n",
      "densenet121\n",
      "test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       250\n",
      "           1       0.91      0.85      0.88       204\n",
      "\n",
      "    accuracy                           0.90       454\n",
      "   macro avg       0.90      0.89      0.89       454\n",
      "weighted avg       0.90      0.90      0.90       454\n",
      "\n",
      "0.956862745098039\n",
      "[[233  17]\n",
      " [ 30 174]]\n"
     ]
    }
   ],
   "source": [
    "mode = ['validation', 'test']\n",
    "for m in mode:\n",
    "    \n",
    "    truth, pred, prob =  inference(better_model, loader[m], 'cpu')\n",
    "    report = sklearn.metrics.classification_report(y_true=truth, y_pred=pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(truth, prob[:,1])\n",
    "    fusetable = sklearn.metrics.confusion_matrix(y_true=truth, y_pred=pred)\n",
    "    print(model_name)\n",
    "    print(m)\n",
    "    print(report)\n",
    "    print(auc)\n",
    "    print(fusetable)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dentalbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fd9f49eb449caab939c5f758b89916c6cadd58c90df6eb1a5b666be347280ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
